

# ğŸ©º Medical Chatbot (RAG-based AI Assistant)

An **AI-powered Medical Chatbot** built using **Retrieval-Augmented Generation (RAG)** that answers medical questions based on a **preloaded medical PDF**.
The application is containerized with **Docker**, deployed on **AWS EC2**, and uses **GitHub Actions for CI/CD**.

---

## ğŸš€ Features

* ğŸ” **RAG Architecture** (LangChain)
* ğŸ“„ Uses **preloaded medical PDF** (no user upload required)
* ğŸ§  **OpenAI Embeddings + GPT-4o**
* ğŸ§¾ **Pinecone Vector Database**
* âš¡ **FastAPI backend**
* ğŸ¨ **HTML/CSS frontend**
* ğŸ³ **Dockerized**
* â˜ï¸ **Deployed on AWS EC2**
* ğŸ”„ **CI/CD with GitHub Actions**

---

## ğŸ—ï¸ Tech Stack

| Layer            | Technology                      |
| ---------------- | ------------------------------- |
| Backend          | FastAPI                         |
| Frontend         | HTML, CSS                       |
| LLM              | OpenAI (GPT-4o)                 |
| Embeddings       | OpenAI `text-embedding-3-small` |
| Vector DB        | Pinecone                        |
| RAG Framework    | LangChain                       |
| Containerization | Docker                          |
| Cloud            | AWS EC2 + ECR                   |
| CI/CD            | GitHub Actions                  |

---

## ğŸ“ Project Structure

```
medical_chatbot/
â”œâ”€â”€ app.py                  # FastAPI entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ service.py          # MedicalChatbotService (RAG logic)
â”‚   â”œâ”€â”€ chat.py
â”‚   â”œâ”€â”€ helper.py
â”‚   â””â”€â”€ prompt.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Medical_book.pdf    # Default medical knowledge source
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html          # Chat UI
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ bot.png
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ cicd.yaml
â””â”€â”€ README.md
```

---

## ğŸ§  How It Works (RAG Flow)

1. Medical PDF is **loaded & chunked**
2. Chunks are converted into **embeddings**
3. Embeddings are stored in **Pinecone**
4. User question â†’ relevant chunks retrieved
5. GPT-4o generates answer using retrieved context

---

## âš™ï¸ Environment Variables

Create a `.env` file (local only):

```env
OPENAI_API_KEY=your_openai_key
PINECONE_API_KEY=your_pinecone_key
```

âš ï¸ **Do not commit `.env` to GitHub**

---

## â–¶ï¸ Run Locally (Without Docker)

```bash
pip install -r requirements.txt
uvicorn app:app --reload
```



## ğŸ³ Run with Docker (Local)

```bash
docker build -t medical-chatbot .
docker run -p 8080:8080 \
  -e OPENAI_API_KEY=your_key \
  -e PINECONE_API_KEY=your_key \
  medical-chatbot
```


## â˜ï¸ Deployment (AWS)

### Deployment Architecture

```
GitHub â†’ GitHub Actions â†’ AWS ECR â†’ EC2 â†’ Docker Container â†’ FastAPI App
```

### CI/CD Pipeline

* **CI**: Build Docker image & push to ECR
* **CD**: Self-hosted runner on EC2 pulls image & restarts container

---



## ğŸ” Security Notes

* API keys stored as **GitHub Secrets**
* EC2 runner uses **IAM Role**
* `.env` and secrets are excluded via `.gitignore` & `.dockerignore`



---

## ğŸ“Œ Future Improvements

* HTTPS with Nginx + SSL
* Domain name integration
* Chat history persistence
* Authentication
* Streaming responses
* Multi-document support

---

## ğŸ‘¨â€ğŸ’» Author

**Yella Reddy**
AI / ML Engineer
ğŸ”— GitHub: [https://github.com/yellareddy19](https://github.com/yellareddy19)


